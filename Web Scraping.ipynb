{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b73a2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.9.0-py2.py3-none-any.whl (277 kB)\n",
      "     -------------------------------------- 277.2/277.2 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting Twisted>=18.9.0\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "     ---------------------------------------- 3.1/3.1 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting cryptography>=3.4.6\n",
      "  Downloading cryptography-41.0.1-cp37-abi3-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting pyOpenSSL>=21.0.0\n",
      "  Downloading pyOpenSSL-23.2.0-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 59.0/59.0 kB ? eta 0:00:00\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=18.1.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting zope.interface>=5.1.0\n",
      "  Downloading zope.interface-6.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "     -------------------------------------- 204.1/204.1 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (65.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (23.0)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.3/93.3 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting lxml>=4.3.0\n",
      "  Downloading lxml-4.9.2-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "     ---------------------------------------- 3.8/3.8 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=3.4.6->scrapy) (1.15.1)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (22.2.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "     -------------------------------------- 74.6/74.6 kB 460.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.5.0)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2\n",
      "  Downloading twisted_iocpsupport-1.0.3-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tldextract->scrapy) (2.28.2)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\khalil\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.12.7)\n",
      "Installing collected packages: twisted-iocpsupport, PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, lxml, jmespath, itemadapter, hyperlink, filelock, cssselect, Automat, Twisted, requests-file, parsel, cryptography, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-15.1.0 cryptography-41.0.1 cssselect-1.2.0 filelock-3.12.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 lxml-4.9.2 parsel-1.8.1 protego-0.2.1 pyOpenSSL-23.2.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.9.0 service-identity-21.1.0 tldextract-3.4.4 twisted-iocpsupport-1.0.3 w3lib-2.1.1 zope.interface-6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "697bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97011d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XPath string equivalent to the CSS Locator\n",
    "xpath = '/html/body/span[1]//a'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'html > body > span:nth-of-type(1) a'\n",
    "\n",
    "# Create the XPath string equivalent to the CSS Locator\n",
    "xpath = '//div[@id=\"uid\"]/span//h4'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'div#uid > span h4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acbd419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <div>HELLO</div>\n",
    "        <div>\n",
    "            <p>GOODBYE</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span>\n",
    "                <p>NOPE</p>\n",
    "                <p>ALMOST</p>\n",
    "                <p>YOU GOT IT!</p>\n",
    "            </span>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "sel = Selector(text=html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3977f06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector query='./span/p[3]' data='<p>YOU GOT IT!</p>'>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain together xpath methods to select desired p element\n",
    "sel.xpath('//div').xpath('./span/p[3]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b1dc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <div>Div 1: <p>paragraph 1</p></div>\n",
    "        <div>Div 2: <p>paragraph 2</p> <p>paragraph 3</p> </div>\n",
    "        <div>Div 3: <p>paragraph 4</p> <p>paragraph 5</p> <p>paragraph 6</p></div>\n",
    "        <div>Div 4: <p>paragraph 7</p></div>\n",
    "        <div>Div 5: <p>paragraph 8</p></div>\n",
    "    </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d9dae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_elements( css ):\n",
    "    sel = Selector( text = data )\n",
    "    print( len(sel.css( css )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "936a4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all.html', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a989d109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector query='//div' data='<div>Div 1: <p>paragraph 1</p></div>'>,\n",
       " <Selector query='//div' data='<div>Div 2: <p>paragraph 2</p> <p>par...'>,\n",
       " <Selector query='//div' data='<div>Div 3: <p>paragraph 4</p> <p>par...'>,\n",
       " <Selector query='//div' data='<div>Div 4: <p>paragraph 7</p></div>'>,\n",
       " <Selector query='//div' data='<div>Div 5: <p>paragraph 8</p></div>'>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Selector selecting html as the HTML document\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Create a SelectorList of all div elements in the HTML document\n",
    "divs = sel.xpath('//div')\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "699d6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1020 elements in the HTML document.\n",
      "You have found:  1020\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "html = requests.get(url).content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Print out the number of elements in the HTML document\n",
    "print(\"There are 1020 elements in the HTML document.\")\n",
    "print(\"You have found: \", len(sel.xpath('//*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "801fc45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Create a selector from the html (of a secret website)\n",
    "sel = Selector(text=data)\n",
    "\n",
    "# Fill in the blank\n",
    "css_locator = ' div.course-block > a'\n",
    "\n",
    "# Print the number of selected elements\n",
    "how_many_elements(css_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2494f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Create the CSS Locator to all children of the element whose id is uid\n",
    "css_locator = '#uid > *'\n",
    "\n",
    "# Print the number of selected elements\n",
    "how_many_elements(css_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43981416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css('div.course-block > a')\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css('::attr(href)')\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath('./@href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a56f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_xpath( xpath ):\n",
    "    xextr = sel.xpath( xpath ).extract()\n",
    "    return xextr\n",
    "\n",
    "def our_css( css ):\n",
    "    cextr = sel.css( css ).extract()\n",
    "    return cextr\n",
    "\n",
    "def print_results( xpath, css_locator ):\n",
    "    print( \"Your XPath extracts to following:\")\n",
    "    print( our_xpath(xpath) )\n",
    "    print(\"_________________\\n\")\n",
    "    print( \"Your CSS Locator extracts the following:\")\n",
    "    print( our_css(css_locator) )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3259a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your XPath extracts to following:\n",
      "['Here is the ', ' link you want!']\n",
      "_________________\n",
      "\n",
      "Your CSS Locator extracts the following:\n",
      "['Here is the ', ' link you want!']\n"
     ]
    }
   ],
   "source": [
    "with open('text_extract.html', 'r') as file:\n",
    "    html = file.read().replace('\\n', '')\n",
    "\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]/text()'\n",
    "\n",
    "# Create a CSS Locator string to the desired text\n",
    "css_locator = 'p#p3::text'\n",
    "\n",
    "# Print the text from our selection\n",
    "print_results(xpath, css_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abeaf36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your XPath extracts to following:\n",
      "['Here is the ', 'DataCamp', ' link you want!']\n",
      "_________________\n",
      "\n",
      "Your CSS Locator extracts the following:\n",
      "['Here is the ', 'DataCamp', ' link you want!']\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]//text()'\n",
    "\n",
    "# Create an CSS Locator string to the desired text\n",
    "css_locator = 'p#p3 ::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print_results(xpath, css_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c089c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_url_title( url, title ):\n",
    "    print( \"Here is what you found:\" )\n",
    "    print( \"\\t-URL: %s\" % url )\n",
    "    print( \"\\t-Title: %s\" % title )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b5e253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.http.response.text import TextResponse\n",
    "\n",
    "response = TextResponse(url='https://www.datacamp.com/courses/all', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d729aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is what you found:\n",
      "\t-URL: https://www.datacamp.com/courses/all\n",
      "\t-Title: None\n"
     ]
    }
   ],
   "source": [
    "# GEt the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title or the website loaded in response\n",
    "this_title = response.xpath('//title/text()').extract_first()\n",
    "\n",
    "# Print out our findings\n",
    "print_url_title(this_url, this_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54501c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
